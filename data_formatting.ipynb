{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_formatting.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1QmSjtMMPpVBpXvDEC3ltai6tlSXFVl6D","authorship_tag":"ABX9TyOzRelUoa4f/sJ8CjImSAkw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data formatting\n","\n","In this script the data is formatted to fit the different models and variable settings used."],"metadata":{"id":"CqePa89Zb8O2"}},{"cell_type":"code","source":["#set the parameters\n","TIME_STEPS = 60   #positive integer, set to 1 for no lag\n","TEST_SPLIT = 0.2\n","LOCATION = 'lobith'\n","\n","#import libraries\n","import pandas as pd\n","import numpy as np\n","import pickle\n","from datetime import datetime\n","from sklearn.preprocessing import StandardScaler\n","\n","#set input and output folders\n","in_folder = '/content/drive/MyDrive/ADS/Final Thesis Project/data/'\n","out_folder = f'/content/drive/MyDrive/ADS/Final Thesis Project/temp/formatted_data/{LOCATION}/'\n","\n","#read the data\n","pred = pd.read_csv(in_folder + f'pred_{LOCATION}.csv')\n","q = pd.read_csv(in_folder + f'q_{LOCATION}.csv')\n","\n","#define some more parameters based on previous ones\n","n_obs = pred.shape[0] - (TIME_STEPS - 1)\n","test_window = int(n_obs*TEST_SPLIT)\n","is_lag = TIME_STEPS > 1\n","\n","#define function to find the year day\n","def date_to_yday(date):\n","  datetime_obj = datetime.strptime(date, '%Y-%m-%d')\n","  yday = datetime_obj.timetuple().tm_yday\n","  return yday\n","\n","#add day of the year in predictors\n","pred['year_day'] = pred['datetime'].apply(date_to_yday)\n","\n","#remove the first column (date)\n","pred = pred.iloc[:,1:]\n","\n","if is_lag:\n","  #add the lagged variables to the dataframe\n","  for i, var in enumerate(pred.columns):\n","    for step in range(0, TIME_STEPS - 1):\n","      pred.insert(i*(TIME_STEPS) + 1, \n","                        f'{var}_lag_{TIME_STEPS - 1 - step}', \n","                        pred[var].shift(TIME_STEPS - 1 - step))\n","\n","  #remove the first TIME_STEPS - 1 rows since they will contain NA values\n","  pred = pred.iloc[TIME_STEPS - 1:,:].reset_index(drop=True)\n","  q = q.iloc[TIME_STEPS - 1:,:].reset_index(drop=True)\n","\n","#loop for every different testing sample\n","for i,test_start in enumerate(range(0,n_obs - 1,test_window), start = 1):\n","\n","  #split train and test for predictors vars\n","  X_train = pd.concat([pred.iloc[0:test_start, :], \n","                       pred.iloc[test_start+test_window:, :]]\n","                      ).to_numpy()\n","  X_test = pred.iloc[test_start:test_start+test_window, :].to_numpy()\n","\n","  #split train and test for predicted var\n","  y_train = pd.concat([q.iloc[0:test_start, -1], \n","                       q.iloc[test_start+test_window:, -1]]\n","                      ).to_numpy()\n","  y_test = q.iloc[test_start:test_start+test_window, -1].to_numpy()\n","\n","  #extract the observations\n","  obs = q.iloc[test_start:test_start+test_window, 1].to_numpy()\n","  \n","  #normalize the input\n","  scaler = StandardScaler()\n","  X_train[:,range(0, X_train.shape[1], TIME_STEPS)] = scaler.fit_transform(X_train[:,range(0, X_train.shape[1], TIME_STEPS)])\n","  X_test[:,range(0, X_train.shape[1], TIME_STEPS)] = scaler.transform(X_test[:,range(0, X_train.shape[1], TIME_STEPS)])\n","  for step in range(1,TIME_STEPS):\n","    X_train[:,range(step, X_train.shape[1], TIME_STEPS)] = scaler.transform(X_train[:,range(step, X_train.shape[1], TIME_STEPS)])\n","    X_test[:,range(step, X_train.shape[1], TIME_STEPS)] = scaler.transform(X_test[:,range(step, X_train.shape[1], TIME_STEPS)])\n","    \n","  scaler_params = scaler.get_params()\n","  with open(out_folder + 'lagged_'*is_lag + f'scaler_params_{i}.pkl', 'wb') as f:\n","    pickle.dump(scaler_params, f)\n","  \n","  #save all the data\n","  np.savez(out_folder + 'no_'*(not is_lag) + f'lag_{i}',\n","           X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, obs = obs)\n"],"metadata":{"id":"5g1sRn2mZUn3","executionInfo":{"status":"ok","timestamp":1654849057806,"user_tz":-120,"elapsed":8825,"user":{"displayName":"Oriol Pomarol","userId":"08254266036031937194"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"35f5ec00-4e75-4de2-8979-71cf326770c3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  if self.run_code(code, result):\n"]}]},{"cell_type":"markdown","source":["SUBSET MODE? --- use code below"],"metadata":{"id":"LpHMI4-5kWXq"}},{"cell_type":"code","source":["#subset only the defined variables\n","vars = ['surfaceWaterStorage', 'snowCoverSWE', 'storUppTotal', 'storGroundwater',\n","        'snowFreeWater', 'gwRecharge', 'baseflow']\n","pred_basel = pred_basel[vars]"],"metadata":{"id":"T2o4XK3LYNl0"},"execution_count":null,"outputs":[]}]}